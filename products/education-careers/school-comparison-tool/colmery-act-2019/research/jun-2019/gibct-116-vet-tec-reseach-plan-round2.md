---
# Research Plan Metadata
title: "Research Plan for [Team, Product, Date]"
date: YYYY-MM-DD
last_updated: YYYY-MM-DD
team: "[Team Name]"
product: "[Product Name]"
product_area: "[e.g., authenticated/unauthenticated]"

# Background Context
background:
  problem_statement: "[What problem is your product trying to solve?]"
  product_location: "[Where is this situated on VA.gov?]"
  user_familiarity: "[New product or iteration?]"
  product_brief_url: "[URL]"

# Research Design
methodology: "[e.g., usability testing, semi-structured interviews, card sort]"
research_format: 
  location: remote # Options: remote, in-person, hybrid
  in_person_details:
    facility: "[Location name if applicable]"
    point_of_contact: "[Name if applicable]"
    equipment: "[Equipment details if applicable]"
  moderated: true # Options: true, false
  
# Research Goals & Questions
research_goals:
  - goal_1: "[First research goal]"
  - goal_2: "[Second research goal]"
  - goal_3: "[Third research goal]"

research_questions:
  - "[Research question 1]"
  - "[Research question 2]"
  - "[Research question 3]"
  - "[Research question 4]"
  - "[Research question 5]"

hypotheses:
  - "[Hypothesis 1]"
  - "[Hypothesis 2]"
  - "[Hypothesis 3]"

expected_outcomes: "[How will findings advance the product?]"

# Recruitment & Participants
recruitment:
  recruiting_partner: "Perigean"
  approach: "[e.g., lean maximum variation]"
  
  primary_criteria:
    - "[Must-have criterion 1]"
    - "[Must-have criterion 2]"
    - "[Must-have criterion 3]"
    
  secondary_criteria:
    - "[Nice-to-have criterion 1]"
    - "[Nice-to-have criterion 2]"
    
  screener_questions:
    - question: "[Screener question text]"
      qualifying_response: "[Expected answer]"
      
participants:
  veterans: 0
  caregivers: 0
  dependents: 0
  total_recruited: 0
  completed_sessions_goal: 0
  
# Timeline & Sessions
timeline:
  pilot_date: "YYYY-MM-DD"
  pilot_participant: "[Name]"
  research_dates: "YYYY-MM-DD to YYYY-MM-DD"
  research_review_submission: "YYYY-MM-DD"
  
session_details:
  duration_minutes: 60
  buffer_minutes: 30
  max_sessions_per_day: 0
  
# Strategic Alignment
octo_priorities:
  - objective: "Objective 1"
    key_results: 
      - "[Specific KR if applicable]"
  - objective: "Objective 2"
    key_results:
      - "[Specific KR if applicable]"
      
veteran_journey_phases:
  - "[e.g., Getting Out]"
  - "[e.g., Starting Up]"
    
# Research Repository Tracking
related_research:
  previous_studies: 
    - "[Link to related past research]"
    
tags:
  - "[product-area]"
  - "[methodology]"
  - "[participant-type]"
  - "[research-phase]"
---

# Sprint Research Plan #
## VET TEC Course Comparison using the GIBCT
#### In support of Colmery Act Section 116

June 10 – 19, 2019


#### Problem Statement 
As a Veteran, I want to learn about VET TEC courses and easily compare approved training providers. I’d like to receive an estimate of my VET TEC benefits so I can make an informed decision. 

#### Solution Hypothesis
By presenting current training provider and program information, displaying an estimate of the funds the Veteran will receive, and presenting contextual help to guide the Veteran through the process of using the tool, VA.gov will simplify and improve the process of researching VET TEC training opportunities. Providing Veterans with all of the necessary information will allow them to feel more confident in their ability to compare and understand VET TEC programs and to obtain training in the high tech-industry.

#### Research Questions

The research is designed to answer the following questions:
 
* How do Veterans engage with the comparison tool landing page prototypes?  
* Of the prototypes tested, is there a clear preference for one or the other? 
* Are Veterans able to find the VET TEC program and filter the GIBCT results to VET TEC providers easily & effectively? 
* What do Veterans understand about their Benefits Estimate and VET TEC program payments?
*	Are Veterans able to compare VET TEC providers and learn about approved programs?
* Does information on VET TEC providers and programs within the GIBCT allow  Veterans to make informed decisions about programs that meet their needs?  


#### What knowledge will make us feel like the research process has been successful?

Understanding whether Veterans are able to easily find the VET TEC program within the Comparison Tool and whether Veterans have the information they need to make decisions about high tech training programs and validating the design solution addresses issues uncovered during the initial testing activities will show our research process has been successful.

#### What kinds of users do we need to talk to answer our questions?

The following users have been identified:
*	Veterans

#### What specific questions do we need to ask on our Usability Testing screener to get the right kinds of users to test the form?

In addition to the standard questions, which ensure a representative mix of participants, the screener should include the following questions and responses:

1.	Have you applied for VA educational benefits before? 
    * If yes, to Question 3
    * If no, to Question 2
2.	Are you interested in applying for VA educational benefits?
    * If yes, to Question 4
    * If no, end
3.	Do you have any VA educational benefits remaining? 
    * If yes, to Question 4
    * If no, end
4.	Are you interested in pursuing education in the high-tech industry? 
    * If high-tech, to Question 5
    * If no, end
5.	What is your age range?  
    * If 22-45 years old, would you like to participate?
    * If other, end

#### What kind of user research do we want to conduct?

The following research methods will be used:

Remote Moderated Usability Testing of a revised high-fidelity prototypes with:
*	8-10 Veterans (Recruit 10)


#### What testing and recording tools do we need to run the research sessions?

Remote Interviews will be recorded via WebEx
Usability test sessions will be recorded via WebEx
In-person notes will be taken by hand and/or on a computer. 

#### What artifacts do we need to support the research effort?

The following artifacts will be developed:
*	High-fidelity, clickable InVision prototypes
*	Veteran Test Script

The InVision prototype will be ready by: June 13

The Test scripts will be ready by: June 13, 2019


#### Who will fill the team roles?
* Screener writers: Theresa McMurdo, Amy Knox
* Recruiter: Perigean Recruiting Firm
* Conversation guide writer: Theresa McMurdo, Amy Knox
* Prototype Designer: Cindy Cruz
* Moderator: Theresa McMurdo, Amy Knox
* Note-taker: Theresa McMurdo, Amy Knox and Cindy Cruz
* Observers: UX Team
* Research readout writer: Theresa McMurdo or Amy Knox

#### When do we want to conduct the testing?

Usability Testing of the Revised Prototype 
*	Usability Testing will be conducted - June 14, 17 & 18, 2019
*	Usability Testing timeslots will be: 
     * June 14: 11AM, 1PM, 3PM 
     * June 17: 11AM, 1PM, 3PM, 5PM 
     * June 18: 9AM, 11AM, 1PM, 3PM, 5PM
*	Each session will last no longer than 45 minutes
*	A debrief among team members will be held immediately after each session.
*	The findings from usability testing will be synthesized once all sessions have occurred and a findings report will be made.

#### When do we need to start recruiting?

Usability Testing Recruiting needs to be complete by: June 13, 2019
