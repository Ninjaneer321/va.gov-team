---
# Research Plan Metadata
title: "Research Plan for [Team, Product, Date]"
date: YYYY-MM-DD
last_updated: YYYY-MM-DD
team: "[Team Name]"
product: "[Product Name]"
product_area: "[e.g., authenticated/unauthenticated]"

# Background Context
background:
  problem_statement: "[What problem is your product trying to solve?]"
  product_location: "[Where is this situated on VA.gov?]"
  user_familiarity: "[New product or iteration?]"
  product_brief_url: "[URL]"

# Research Design
methodology: "[e.g., usability testing, semi-structured interviews, card sort]"
research_format: 
  location: remote # Options: remote, in-person, hybrid
  in_person_details:
    facility: "[Location name if applicable]"
    point_of_contact: "[Name if applicable]"
    equipment: "[Equipment details if applicable]"
  moderated: true # Options: true, false
  
# Research Goals & Questions
research_goals:
  - goal_1: "[First research goal]"
  - goal_2: "[Second research goal]"
  - goal_3: "[Third research goal]"

research_questions:
  - "[Research question 1]"
  - "[Research question 2]"
  - "[Research question 3]"
  - "[Research question 4]"
  - "[Research question 5]"

hypotheses:
  - "[Hypothesis 1]"
  - "[Hypothesis 2]"
  - "[Hypothesis 3]"

expected_outcomes: "[How will findings advance the product?]"

# Recruitment & Participants
recruitment:
  recruiting_partner: "Perigean"
  approach: "[e.g., lean maximum variation]"
  
  primary_criteria:
    - "[Must-have criterion 1]"
    - "[Must-have criterion 2]"
    - "[Must-have criterion 3]"
    
  secondary_criteria:
    - "[Nice-to-have criterion 1]"
    - "[Nice-to-have criterion 2]"
    
  screener_questions:
    - question: "[Screener question text]"
      qualifying_response: "[Expected answer]"
      
participants:
  veterans: 0
  caregivers: 0
  dependents: 0
  total_recruited: 0
  completed_sessions_goal: 0
  
# Timeline & Sessions
timeline:
  pilot_date: "YYYY-MM-DD"
  pilot_participant: "[Name]"
  research_dates: "YYYY-MM-DD to YYYY-MM-DD"
  research_review_submission: "YYYY-MM-DD"
  
session_details:
  duration_minutes: 60
  buffer_minutes: 30
  max_sessions_per_day: 0
  
# Strategic Alignment
octo_priorities:
  - objective: "Objective 1"
    key_results: 
      - "[Specific KR if applicable]"
  - objective: "Objective 2"
    key_results:
      - "[Specific KR if applicable]"
      
veteran_journey_phases:
  - "[e.g., Getting Out]"
  - "[e.g., Starting Up]"
    
# Research Repository Tracking
related_research:
  previous_studies: 
    - "[Link to related past research]"
    
tags:
  - "[product-area]"
  - "[methodology]"
  - "[participant-type]"
  - "[research-phase]"
---

# Research Plan for School Ratings
As a designer I need to create a research plan to share with my team.	 	
## Goals	
1. What product & team are you doing this research for?	
- Booz Allen research for Education Services / OIT
2. Background: Briefly, what is the background on this product? What would a new person on the team need to know about this product? 	

When Veterans are looking for schools within the Comparison tool, they want to know about the experiences of other Veterans to form a better understanding of what their personal experience at the school would be like. Ratings enable Veterans to quickly assess the suitability of a school (both pro and con), bolstering their confidence and level of comfort in deciding if a particular school is right for them.

3. Research questions: What question(s) do you hope to be able to answer after completing this research? 	

- What do users think of the star ratings?
- Who do users think are providing the ratings?
- How does the number of ratings weigh into a user's perception of a particular school?
- Are users interested in the different rating categories or just the overall score?
- Which categories are valuable to users?  Are there any categories that aren't valuable?  Are there any additional categories they would like to see?
- What do users think the different categories mean?
- How do users think the rating scores are determined (average of submitted scores, calculated value, etc.)?
- Would ratings affect the school selection process?
- Do users think the ratings are useful?
- How do users think ratings are collected?
- How trustworthy are the ratings?
- What level of privacy do users expect if they would provide a rating?

4. Hypothesis: What is your hypothesis for this research? 

We believe that Veterans will understand the rating categories and find the ratings useful, especially in determining the Veteran experience at a particular school.

## Method	
1.	What method of research are you planning? 	
  - Remote moderated usability testing	
  	
2.	Why this method? How does this methodology help you answer your research questions? 	

Researchers need to be able to see how users react and interact with school ratings.  We need to be able to ask about their impressions and expectations of those ratings to determine if the current design is meeting their needs.  We also need to identify any points of confusion and whether this feature will be valuable to Veterans.

3.	Where are you planning to do your research? 

 - Online using Zoom
 
4.	What will you be testing? 
 - Prototype and content
 
5.  If remote: What tool do you plan to use (Zoom, GoToMeeting, Webex)	
 - Zoom
 
## Participants and Recruitment	
1.	Participant criteria: What are you looking for in a participant?	  
(Mention: Number of people, ages, accessibility preferences, geographical diversity, login requirements, VA benefit requirements, familiarity with technology, etc. Keep in mind, the more requirements, the more difficult the recruit, so give ample time to ensure the right participant mix.)	
* 10 participants total
  * Actively looking for a school to attend for themselves where they'll use their VA education benefits **or** 
  * In the last 6 months, looked for a school to attend for themselves where they'd use their VA education benefits
* GI Bill beneficiaries
* Age range: 18-45
* Gender: Mix
* Geography: Geographically dispersed across US
* VA Benefit requirement:
  * 6 using or planning to use Chapter 33 Post-9/11 benefits in the next 3 months
  * 4 using or planning to use education benefits other than Chapter 33 Post-9/11 in the next 3 months
* Technology: Access to a computer, preferably running Google Chrome browser
* Familiarity with technology: Any
* Login requirements: None

2.	What is your recruitment strategy? 	
- Recruitment will be performed by Perigean Technologies	

## When? 	
1.	Timeline: What dates do you plan to do research? 	
(IF you are using the research recruiting contract, please submit 1 FULL week prior to the start of research for remote, 2+ weeks for in person.)  
* Testing sessions: Thur, 8/27/20 to Fri 8/28/20     
2.	Prepare: When will the thing you are testing be ready? (Goes without saying, but should be a few days before testing will begin.) 
* By COB: Thur, 8/20/20
3. Length of Sessions: How long do you estimate each session will be? 
  - 45 minutes
4.	Availability: If applicable, when would you like sessions scheduled? 
 - Thursday, Aug 27:  8:30-9:15AM; 11-11:45AM; 12:15-1PM; 1:15-2PM; 3-3:45PM
 - Friday, Aug 28: 8:30-9:15AM; 9:15-10AM; 11:30AM-12:15PM; 1:15-2PM; 3-3:45PM
5.	Pilot: Please indicate a date before your sessions begin for piloting your research. Which member of the design team will you pilot your research with? 
* Wed, 8/26/20    
## Team Roles	
Please list the people who will be serving in each role. **Include the primary phone number for moderator and the emails for moderator, notetaker, and observers. If you need Perigean to take notes for you, indicate that next to Notetaker** 	
- Moderator:	Amy Knox; 301.254.0907; knox_amy@bah.com
- Research guide writing and task development (usually but not always same as moderator):	Booz Allen UX team
- Participant recruiting & screening:	Perigean Technologies
- Project point of contact:	Amy Knox
- Prototype designer: Cindy Cruz
- Participant(s) for pilot test:	
- Note-takers:	Cindy Cruz cruz_cindy@bah.com; Jen Jones jones_jennifer2@bah.com
- Observers:	Brian Grubb brian.grubb@va.gov; Desiree Turner turner_desiree@bah.com; Joe Preisser joseph.preisser@va.gov; Joe Welton joseph.welton@va.gov; Will McCormack mccormack_will@bah.com; Lauren Anderson lauren.alexanderson@va.gov; Lacey Higley lacey.higley@va.gov; Matt Self matthew.self2@va.gov; Dan Shawkey shawkey_daniel@bah.com; Darla VanNieukerk darla.vannieukerk@va.gov; Tammy Hurley tammy.hurley1@va.gov; Darrell Neel neel_darrell@bah.com;Luke Tickner Lucas.Tickner@va.gov	
## Resources	
- Project Brief: 	
https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/education-careers/school-comparison-tool/school-ratings/product-outline.md

- Convo Guide: https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/education-careers/school-comparison-tool/school-ratings/research/discussion-guide2.md	 
- Synthesis	
https://app.mural.co/t/bahdigitalexperience6902/m/bahdigitalexperience6902/1596203507651/dc70997c2bc44977e499d367d36e419c2541a5f0?sender=jonesjennifer26349 	
- Lessons Learned	
*Did you have any takeaways from the process of this research round that you want the team to remember for the future? Document them here.* 	
- Read-Out/Results	
  - https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/education-careers/school-comparison-tool/school-ratings/research/Ratings-Research-Readout-202009.pdf
