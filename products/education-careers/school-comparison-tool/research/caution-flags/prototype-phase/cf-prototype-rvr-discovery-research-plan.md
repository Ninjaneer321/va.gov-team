---
# Research Plan Metadata
title: "Research Plan for [Team, Product, Date]"
date: YYYY-MM-DD
last_updated: YYYY-MM-DD
team: "[Team Name]"
product: "[Product Name]"
product_area: "[e.g., authenticated/unauthenticated]"

# Background Context
background:
  problem_statement: "[What problem is your product trying to solve?]"
  product_location: "[Where is this situated on VA.gov?]"
  user_familiarity: "[New product or iteration?]"
  product_brief_url: "[URL]"

# Research Design
methodology: "[e.g., usability testing, semi-structured interviews, card sort]"
research_format: 
  location: remote # Options: remote, in-person, hybrid
  in_person_details:
    facility: "[Location name if applicable]"
    point_of_contact: "[Name if applicable]"
    equipment: "[Equipment details if applicable]"
  moderated: true # Options: true, false
  
# Research Goals & Questions
research_goals:
  - goal_1: "[First research goal]"
  - goal_2: "[Second research goal]"
  - goal_3: "[Third research goal]"

research_questions:
  - "[Research question 1]"
  - "[Research question 2]"
  - "[Research question 3]"
  - "[Research question 4]"
  - "[Research question 5]"

hypotheses:
  - "[Hypothesis 1]"
  - "[Hypothesis 2]"
  - "[Hypothesis 3]"

expected_outcomes: "[How will findings advance the product?]"

# Recruitment & Participants
recruitment:
  recruiting_partner: "Perigean"
  approach: "[e.g., lean maximum variation]"
  
  primary_criteria:
    - "[Must-have criterion 1]"
    - "[Must-have criterion 2]"
    - "[Must-have criterion 3]"
    
  secondary_criteria:
    - "[Nice-to-have criterion 1]"
    - "[Nice-to-have criterion 2]"
    
  screener_questions:
    - question: "[Screener question text]"
      qualifying_response: "[Expected answer]"
      
participants:
  veterans: 0
  caregivers: 0
  dependents: 0
  total_recruited: 0
  completed_sessions_goal: 0
  
# Timeline & Sessions
timeline:
  pilot_date: "YYYY-MM-DD"
  pilot_participant: "[Name]"
  research_dates: "YYYY-MM-DD to YYYY-MM-DD"
  research_review_submission: "YYYY-MM-DD"
  
session_details:
  duration_minutes: 60
  buffer_minutes: 30
  max_sessions_per_day: 0
  
# Strategic Alignment
octo_priorities:
  - objective: "Objective 1"
    key_results: 
      - "[Specific KR if applicable]"
  - objective: "Objective 2"
    key_results:
      - "[Specific KR if applicable]"
      
veteran_journey_phases:
  - "[e.g., Getting Out]"
  - "[e.g., Starting Up]"
    
# Research Repository Tracking
related_research:
  previous_studies: 
    - "[Link to related past research]"
    
tags:
  - "[product-area]"
  - "[methodology]"
  - "[participant-type]"
  - "[research-phase]"
---

# Caution Flag Prototype & Reduce Veteran Risk Discovery - Research Plan

Feb 18, 2019 – March 13, 2020

### Background

Veterans continue to attend schools that may be a risky use of their GI Bill benefits.  VA collects an array of cautionary information on schools that they report in GI Bill Comparison Tool (GIBCT).  

Within the GIBCT, military-connected students continue to view profiles of institutions that have increased regulatory or legal scrutiny, even when those schools are flagged with “Caution” labels. They may apply to and even attend the institutions without being aware of these potential hazards.  

Outside the GIBCT, Veterans are not getting information about these potentially risky educational investments.  


### Problem Statement 

As a GI Bill beneficiary, I'd like to make good use of my education benefits and receive a high-quality education.  

As a beneficiary who is using the GI Bill Comparison Tool to compare schools, I'd like to know which schools have received cautionary warnings and may pose potential risks, so I don’t waste, or even lose, my benefits attending an institution that’s known to have critical issues.  


### Solution Hypothesis

By better understanding how GI Bill beneficiaries anticipate VA would alert them of flagged schools via mechanisms other than the GI Bill Comparison Tool, we can make recommendations on how to better notify beneficiaries of the potential dangers of relevant schools.  

By enhancing the design and content of the caution flags that appear on the Search Results and School Profile pages of the GI Bill Comparison Tool, potential GI Bill Beneficiaries will be deterred from considering schools with cautionary warnings. In addition, the content changes will help GI Bill beneficiaries better understand the nature of the cautionary warnings.  

### Research Questions

The research is designed to answer the following questions:

#### Reduce Veteran risk during school section
Reduce Veteran risk during school section
* How do Veterans think of their GI Bill Benefits? Do they consider them an investment asset or is there another characterization that’s more accurate?
* Where would Veterans look to find information from VA on the schools that are risky investments of GI Bill Benefits?
* What are Veterans thoughts on the responsibility of VA to alert them if a school they were planning to attend or are attending was flagged as a risk?
* How would Veterans expect VA might alert them if the school they were planning to attend or were attending was flagged as risky?
* Why might a Veteran choose to attend a school that VA knows to be a risky use of GI Bill benefits due to performance or other issues?

#### Caution Flag on Search Results page and School Profile page
* Do the updated alerts deter GI Bill beneficiaries from exploring schools with caution flags?
* What, if any, design modifications would increase the effectiveness of the caution flags?
* Under what circumstances do GI Bill beneficiaries wish to click through to the school profile page to learn more about the warnings?
* Which flags, if any, are less of a deterrent to GI Bill beneficiaries?
* Do GI Bill beneficiaries feel they receive the necessary information at the moment they need it?
* What questions do GI Bill beneficiaries have regarding the meaning of the caution flag content?
* Are GI Bill beneficiaries interested in following the links to learn more about the cautions?


### What knowledge will make us feel like the research process has been successful?

Understanding Veterans’ expectations regarding VA’s responsibility to alert them to flagged schools.  Observing that participants are quickly deterred from exploring schools with cautionary warnings as they engage with the GI Bill Comparison Tool will make us feel like the research process has been successful.

### What kinds of users do we need to talk to answer our questions?

The following users have been identified:
* GI Bill beneficiaries who are interested in obtaining an undergraduate degree and have not yet enrolled in a school.
   * 7 Veterans
   * 3 Dependents  

### What specific questions do we need to ask on our Usability Testing screener to get the right kinds of users to test the form?

In addition to the standard questions, which ensure a representative mix of participants, the screener should include the following questions and responses.

### Screener questions

1.	Are you a Veteran or a family member or both?  
    * If more of this type of user is needed, to Question 2        
    * If not, end call  
2.  Are you eligible for GI Bill education benefits?  
    * If yes, to Question 3  
    * If no, end call  
3.	Are you planning to use your GI Bill education benefits?
    *	If yes, to Question 4
    * If no, end call
4.	When are you planning to begin using your GI Bill education benefits?
    * Have already started, end call 
    * Not sure, would you like to participate?
    * Up to the next 23 months, would you like to participate?
    *	23 months or beyond, end call


### What kind of user research do we want to conduct?

The following research methods will be used:

* Remote usability testing of a prototype with 6-8 GI Bill Beneficiaries

### What testing and recording tools do we need to run the research sessions?

Usability test sessions will be recorded via Zoom.

In-person notes will be taken by hand and/or on a computer.

### What artifacts do we need to support the research effort?

The following artifacts will be developed:
* Veteran Discussion Guide  
* Interactive Prototype

The Discussion Guide will be ready by: February 25, 2020.
The Interactive Prototype will be ready by: March 2, 2020.

### Who will fill the team roles?

* Screener writers: UX Team
* Recruiter: Perigean Recruiting Firm
* Discussion guide writer: UX team
* Moderator: Amy Knox
* Prototype designer: Cindy Cruz
* Note-taker: Cindy Cruz, Jen Jones
* Observers: 
  * DEPO: Matt Self
  * Education Service: Joseph Preisser, Brian Grubb, Tammy Hurley
  * OIT: Darla van Nieukerk
  * Booz Allen: Will McCormack, Dan Shawkey, Desiree Turner
* Research readout writer: UX Team

### When do we want to conduct the testing?

Usability Testing will be conducted on March 2nd, 3rd & 4th, 2020.

Timeslots will be: 
*	Mar 2: 1PM, 3PM
*	Mar 3: 9AM, 11AM, 1PM, 3PM, 5PM 
*	Mar 4: 9AM, 11AM, 1PM, 3PM


Each session will last no longer than 45 minutes.  

A debrief among team members will be held immediately after each session.  

Findings from will be synthesized once all sessions have occurred and a findings report will be made.  

### When will recruiting occur?

Recruiting needs to be complete by: March 2, 2020
