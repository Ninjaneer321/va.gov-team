---
# Research Plan Metadata
title: "Research Plan for [Team, Product, Date]"
date: YYYY-MM-DD
last_updated: YYYY-MM-DD
team: "[Team Name]"
product: "[Product Name]"
product_area: "[e.g., authenticated/unauthenticated]"

# Background Context
background:
  problem_statement: "[What problem is your product trying to solve?]"
  product_location: "[Where is this situated on VA.gov?]"
  user_familiarity: "[New product or iteration?]"
  product_brief_url: "[URL]"

# Research Design
methodology: "[e.g., usability testing, semi-structured interviews, card sort]"
research_format: 
  location: remote # Options: remote, in-person, hybrid
  in_person_details:
    facility: "[Location name if applicable]"
    point_of_contact: "[Name if applicable]"
    equipment: "[Equipment details if applicable]"
  moderated: true # Options: true, false
  
# Research Goals & Questions
research_goals:
  - goal_1: "[First research goal]"
  - goal_2: "[Second research goal]"
  - goal_3: "[Third research goal]"

research_questions:
  - "[Research question 1]"
  - "[Research question 2]"
  - "[Research question 3]"
  - "[Research question 4]"
  - "[Research question 5]"

hypotheses:
  - "[Hypothesis 1]"
  - "[Hypothesis 2]"
  - "[Hypothesis 3]"

expected_outcomes: "[How will findings advance the product?]"

# Recruitment & Participants
recruitment:
  recruiting_partner: "Perigean"
  approach: "[e.g., lean maximum variation]"
  
  primary_criteria:
    - "[Must-have criterion 1]"
    - "[Must-have criterion 2]"
    - "[Must-have criterion 3]"
    
  secondary_criteria:
    - "[Nice-to-have criterion 1]"
    - "[Nice-to-have criterion 2]"
    
  screener_questions:
    - question: "[Screener question text]"
      qualifying_response: "[Expected answer]"
      
participants:
  veterans: 0
  caregivers: 0
  dependents: 0
  total_recruited: 0
  completed_sessions_goal: 0
  
# Timeline & Sessions
timeline:
  pilot_date: "YYYY-MM-DD"
  pilot_participant: "[Name]"
  research_dates: "YYYY-MM-DD to YYYY-MM-DD"
  research_review_submission: "YYYY-MM-DD"
  
session_details:
  duration_minutes: 60
  buffer_minutes: 30
  max_sessions_per_day: 0
  
# Strategic Alignment
octo_priorities:
  - objective: "Objective 1"
    key_results: 
      - "[Specific KR if applicable]"
  - objective: "Objective 2"
    key_results:
      - "[Specific KR if applicable]"
      
veteran_journey_phases:
  - "[e.g., Getting Out]"
  - "[e.g., Starting Up]"
    
# Research Repository Tracking
related_research:
  previous_studies: 
    - "[Link to related past research]"
    
tags:
  - "[product-area]"
  - "[methodology]"
  - "[participant-type]"
  - "[research-phase]"
---

# Sprint Research Plan #
## VET TEC Course Comparison using the GIBCT
#### In support of Colmery Act Section 116

May 23, 2019 – June 7, 2019


#### Problem Statement 
As a Veteran, I want to learn about VET TEC courses and easily compare approved training providers. I’d like to receive an estimate of my VET TEC benefits so I can make an informed decision. 

#### Solution Hypothesis
By presenting current training provider and program information, displaying an estimate of the funds the Veteran will receive, and presenting contextual help to guide the Veteran through the process of using the tool, VA.gov will simplify and improve the process of researching VET TEC training opportunities. Providing Veterans with all of the necessary information will allow them to feel more confident in their ability to compare and understand VET TEC programs and to obtain training in the high tech-industry.

#### Research Questions

The research is designed to answer the following questions:
 
* How do Veterans engage with the comparison tool to learn about VET TEC providers and approved programs? 
* Which data are Veterans are most interested in as they decide on a VET TEC program/provider? 
* What additional data would make comparing VET TEC providers easier for Veterans?
*	What challenges or common pitfalls do Veterans face when attempting to compare VET TEC providers and learn about approved programs?
* What are the Veterans' assumptions about the Benefits Estimate? 
*	What additional information or contextual help would ensure that users provide the correct inputs and are able to leverage the tool’s full capabilities?
*	What aspects of the GIBCT Calculator could be improved to enhance usability? 


#### What knowledge will make us feel like the research process has been successful?

Developing a thorough understanding of how Veterans use the Comparison Tool to research VET TEC providers and estimated funding for their program, and validating the design solution addresses issues uncovered during discovery activities will show our research process has been successful.

#### What kinds of users do we need to talk to answer our questions?

The following users have been identified:
*	Veterans

#### What specific questions do we need to ask on our Usability Testing screener to get the right kinds of users to test the form?

In addition to the standard questions, which ensure a representative mix of participants, the screener should include the following questions and responses:

1.	Have you applied for VA educational benefits before? 
    * If yes, to Question 3
    * If no, to Question 2
2.	Are you interested in applying for VA educational benefits?
    * If yes, to Question 4
    * If no, end
3.	Do you have any VA educational benefits remaining? 
    * If yes, to Question 4
    * If no, end
4.	Are you interested in pursuing education in the high-tech industry? 
    * If high-tech, to Question 5
    * If other, determine applicability before Question 5
5.	What is your age range?  
    * If 22-45 years old, would you like to participate?
    * If other, end

#### What kind of user research do we want to conduct?

The following research methods will be used:

Remote Moderated Usability Testing of a revised high-fidelity prototype with:
*	4-6 Veterans (Recruit 6)


#### What testing and recording tools do we need to run the research sessions?

Remote Interviews will be recorded via WebEx
Usability test sessions will be recorded via WebEx
In-person notes will be taken by hand and/or on a computer. 

#### What artifacts do we need to support the research effort?

The following artifacts will be developed:
*	A revised high-fidelity, clickable InVision prototype
*	Veteran Test Script

The InVision prototype will be ready by: May 31, 2019

The Test scripts will be ready by: May 31, 2019


#### Who will fill the team roles?
* Screener writers: Theresa McMurdo, Amy Knox
* Recruiter: Perigean Recruiting Firm
* Conversation guide writer: Theresa McMurdo, Amy Knox
* Prototype Designer: Cindy Cruz
* Moderator: Theresa McMurdo, Amy Knox
* Note-taker: Theresa McMurdo, Amy Knox and Cindy Cruz
* Observers: UX Team
* Research readout writer: Theresa McMurdo or Amy Knox

#### When do we want to conduct the testing?

Usability Testing of the Revised Prototype 
*	Usability Testing will be conducted - June 3rd-4th, 2019
*	Usability Testing timeslots will be: 
     * June 3rd: 11AM, 1PM, 3PM, 5PM
     * June 4th: 9AM, 11AM, 1PM, 3PM, 5PM
*	Each session will last no longer than 45 minutes
*	A debrief among team members will be held immediately after each session.
*	The findings from usability testing will be synthesized once all sessions have occurred and a findings report will be made.

#### When do we need to start recruiting?

Usability Testing Recruiting needs to be complete by: May 31st, 2019
