---
# Research Plan Metadata
title: "Research Plan for [Team, Product, Date]"
date: YYYY-MM-DD
last_updated: YYYY-MM-DD
team: "[Team Name]"
product: "[Product Name]"
product_area: "[e.g., authenticated/unauthenticated]"

# Background Context
background:
  problem_statement: "[What problem is your product trying to solve?]"
  product_location: "[Where is this situated on VA.gov?]"
  user_familiarity: "[New product or iteration?]"
  product_brief_url: "[URL]"

# Research Design
methodology: "[e.g., usability testing, semi-structured interviews, card sort]"
research_format: 
  location: remote # Options: remote, in-person, hybrid
  in_person_details:
    facility: "[Location name if applicable]"
    point_of_contact: "[Name if applicable]"
    equipment: "[Equipment details if applicable]"
  moderated: true # Options: true, false
  
# Research Goals & Questions
research_goals:
  - goal_1: "[First research goal]"
  - goal_2: "[Second research goal]"
  - goal_3: "[Third research goal]"

research_questions:
  - "[Research question 1]"
  - "[Research question 2]"
  - "[Research question 3]"
  - "[Research question 4]"
  - "[Research question 5]"

hypotheses:
  - "[Hypothesis 1]"
  - "[Hypothesis 2]"
  - "[Hypothesis 3]"

expected_outcomes: "[How will findings advance the product?]"

# Recruitment & Participants
recruitment:
  recruiting_partner: "Perigean"
  approach: "[e.g., lean maximum variation]"
  
  primary_criteria:
    - "[Must-have criterion 1]"
    - "[Must-have criterion 2]"
    - "[Must-have criterion 3]"
    
  secondary_criteria:
    - "[Nice-to-have criterion 1]"
    - "[Nice-to-have criterion 2]"
    
  screener_questions:
    - question: "[Screener question text]"
      qualifying_response: "[Expected answer]"
      
participants:
  veterans: 0
  caregivers: 0
  dependents: 0
  total_recruited: 0
  completed_sessions_goal: 0
  
# Timeline & Sessions
timeline:
  pilot_date: "YYYY-MM-DD"
  pilot_participant: "[Name]"
  research_dates: "YYYY-MM-DD to YYYY-MM-DD"
  research_review_submission: "YYYY-MM-DD"
  
session_details:
  duration_minutes: 60
  buffer_minutes: 30
  max_sessions_per_day: 0
  
# Strategic Alignment
octo_priorities:
  - objective: "Objective 1"
    key_results: 
      - "[Specific KR if applicable]"
  - objective: "Objective 2"
    key_results:
      - "[Specific KR if applicable]"
      
veteran_journey_phases:
  - "[e.g., Getting Out]"
  - "[e.g., Starting Up]"
    
# Research Repository Tracking
related_research:
  previous_studies: 
    - "[Link to related past research]"
    
tags:
  - "[product-area]"
  - "[methodology]"
  - "[participant-type]"
  - "[research-phase]"
---

# Research Plan for Find Claimant, Claimant Page and Navigation

June 2025

## Background

The Accredited Representative Facing (ARF) Team is building an Accredited Representative Portal (ARP) that will allow Veteran Service Organization (VSO) representatives to submit online any claim and supporting evidence to the VA. Currently, representatives mail, fax, use QuickSubmit, Stakeholder Enterprise Portal (SEP) or third-party tools to submit claims and evidence.

In this version of ARP that we are testing, representatives search for a specific claimant which then leads them to a Claimant Page. From this page they can submit a claim, view claimant information, view representation requests, or view claimant’s submission history. In this version, the navigation only shows “Find Claimant,” “Representative requests,” and “Help,” with “Your recent history” as part of a dropdown menu underneath the representative’s name.

## Research Goals

* Evaluate the usability of the search. Is it clear and intuitive? What could be improved?
* Evaluate the value of the Claimant Page overview. Is the information useful? What could improve its value?
* Evaluate the clarity and value of the claim submission flow, starting from Claimant Page. What could be improved?
* Evaluate the navigation. Are the sections and their purpose clear?

## Research Questions

Goal: Evaluate the usability of the search. Is it clear and intuitive? What could be improved?

* Do people expect to land on the search page after logging in?
* What do reps search by to find an individual in existing tools? What method do they prefer?
* What do people expect to see next after clicking “search?”

Goal: Evaluate the value of the Claimant Page overview. Is the information useful? What could improve its value?

* Is the information on the claimant overview must have/nice to have/not needed? What other information would be helpful to see?
* Are the alerts relevant and clear? What other alerts/notifications would they want to see for a claimant?

Goal: Evaluate the clarity and value of the claim submission flow, starting from Claimant Page. What could be improved?

* What do they expect to see next after selecting “Start [Form number] submission”?
* Do they understand differences in how submitted files will be processed?
* What would their next step be after submitting files?
* Is this version of submitting a claim better and faster than their current solution? Why or why not?

Goal: Evaluate the navigation. Are the sections and their purpose clear?

* When given scenarios, will a representative know which navigation section to visit, to take action? (Note: The following question(s) would be asked prior to giving the participant scenarios for Find Claimant and Claimant page.)
* Are the section headings clear? How confident do they feel in navigating the app?

## Hypotheses

Representatives will find

* Searching by last four digits of SSN, first name and last name will feel like an appropriate amount of information to enter.
* Information on the claimant overview page helps answer questions the representative has about the claimant; it is useful.
* The solution for submitting the form through Claimant Page is perceived as an equal to QuickSubmit.
* Each participant can complete 75% of the navigation tasks successfully.

## Outcome

This research will inform edits to Find Claimant, Claimant page, and the Claim Submission flow. It will also inform future iterations of claims submission.

## Methodology

1:1 Usability test, including navigation tasks.

### Location

Remote sessions, over Teams

## Recruitment

### Recruitment approach

VSO representatives are our primary audience. We will be utilizing our VSO research panel to fulfill criteria. There will be no screener.

### Recruitment criteria

* 4-5 VSO representatives
* From [our representative panel](https://dvagov.sharepoint.com/%3Ax%3A/r/sites/vaabdvro/_layouts/15/doc2.aspx?sourcedoc=%7B6%5B%E2%80%A6%5Dp_ParticipantPanel.xlsx&action=default&mobileredirect=true)

Primary criteria

* 4-5 people
* Those who at a minimum are familiar with QuickSubmit. They can also be familiar with other third party tools.
* Mix of single and cross-accredited
* Mix of reps from the “Big 6” and county reps

## Timeline

Timeline is an estimation and will be updated if needed. Representative availability might impact this timeline.

### Estimated research length: ~4 to 5 weeks

### Research sessions

* Planned dates of research: Week of 6/2/2025

### Length of sessions

* Session length: 1 hour
* Buffer time between sessions: 30 minutes
* Maximum Sessions per day: 3

### Week of May 19

* Finalize research plan
* Draft and share conversation guide for asynchronous feedback
* Identify who to recruit (expect to email 9-10 reps)
* Hold time on key team member calendars for sessions

### Week of May 26

* Finalize conversation guide
* Finalize prototype for testing
* Send recruitment emails starting 4/27 (day after Memorial day)
* Set up slack channel for this testing session
* Confirm sessions and request for observer + notetaker signups
* Schedule 10 minutes post session for debriefs
* Identify backup moderators

### Week of June 2

* Confirm sessions and request for observer + notetaker signups
* Hold sessions
* Start analysis and synthesis
* If possible, share early findings with internal working team

Week of June 9

* Synthesize data
* Draft report

Week of June 16

* Shareout report with immediate team by June 17, decide on whether or not to share with other stakeholders. Shareout may take place end of the prior week.

## Team Roles

* Moderators: Laura Paradis, Kate Albee, Eva Heintzelman or Jennifer Seipel as backup
* Research guide writing and task development: Laura Paradis, Kate Albee
* Participant recruiting & scheduling: Kate Albee
* Project point of contact: Laura Paradis
* Notetakers: Laura Paradis, Kate Albee, Eva Heintzelman
* Observers: ARF team members (especially those on the claims submission workstream), Enablement team
