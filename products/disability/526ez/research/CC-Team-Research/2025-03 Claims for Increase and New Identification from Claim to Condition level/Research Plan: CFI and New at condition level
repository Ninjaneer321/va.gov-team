2025-03 New/CFI from Claim to Condition Level 
Research Plan
Written by: Sudeepti Bhatnagar and kyra.berman-gestring@agile6.com
Last updated Apr 3, 2025


Note: This research plan was created at the request of OCTO at the end of the Conditions Team’s period of performance. Because of this time constraint certain aspects of this report including the Timeline and Team Roles were out of scope. Impacted data is highlighted in blue throughout this report, to be completed by the implementing team. 

Background
The current 526EZ form process requires Veterans to select whether they are applying for new conditions or seeking an increase (CFI) for previously rated disabilities at the beginning of Step 2 of 5, creating two separate “list and loops” for new and CFI claims. This approach leads to usability issues, including a disconnected data flow and frequent misidentification of claim types. Veterans often confuse when to use CFI versus a supplemental claim, which is particularly problematic as Congress is pushing to allow supplemental claims to be filed through the 526EZ form. In response, the VBA is updating the paper form to associate new and CFI claims with specific conditions rather than handling them at the claim level, aiming for greater compliance and reduced confusion.
To address these challenges and prepare for policy changes, we are exploring two design approaches that allow Veterans to specify whether a condition is new or a CFI at the condition level. These approaches aim to improve usability, align with ongoing VBA efforts to update the paper form, and ensure consistency across platforms. (Read more about the problem space in the Design Brief)


Current State: Veterans have to indicate if a condition is new or has gotten worse in the first question of Step 2


Screenshot of PDF 526EZ Form illustrating how paper form does not separate new conditions from CFI

This research focuses on two key design innovations to solve these issues:
1. Apple Prototype Improvements
Previous testing on the Apple prototype (reference: figma designs) demonstrated promising results using a multiple response multi-page variation. Based on user feedback and policy considerations, we've implemented several targeted updates:
Date Component Modifications: Removed the required asterisk, added a day field to the date entry component, and included hint text showing how to enter approximate dates
Autocomplete Refinements: Reduced examples from 7 to 3 for improved scanability and changed the label from "Add a new condition" to "Select or enter new condition" to encourage use of autosuggestions
Cause Question Improvements: Refined option text for brevity and plain language, added "event" and "onset of a disease" options to align with the paper form and adjudicator manual
Cause Details Screen Enhancements: Incorporated condition names dynamically via formData to provide clear context, reworded hint text for clarity, and improved consistency across cause detail screens
Laterality: While this is not a change from the Apple prototype, we will be testing laterality again. When a Veteran chooses a condition from the autosuggestion that has laterality associated with it, they’ll be prompted with a follow-up question to indicate side of body. 
2. Condition Type Approaches
To address the fundamental issues of claim type misclassification and usability issues, we're testing two distinct design approaches:
Mango Design : Shows all rated disabilities upfront, ensuring Veterans review existing disabilities before making selections. This approach may reduce confusion by making it easier to determine what they're applying for, though it could present challenges in streamlining titles and copy. (Reference: Figma) 

Screenshot of Mango 
Kiwi Design: Separates new conditions from CFI claims, potentially reducing cognitive burden by allowing Veterans to focus on one claim type at a time. This approach may streamline titles and copy but could still face challenges if Veterans don't know their rated disabilities. (Reference: Figma)
 
Screenshot of Kiwi
This research evaluates these design changes to identify the approach that best meets Veterans' needs, aligns with VBA's paper form updates, and minimizes processing errors.
OCTO Priorities
Objective 1: VA’s digital experiences are the easiest and most efficient way to access VA health care and benefits.
Key Result 1: Improve satisfaction with our web and mobile products by 5 points.
Key Result 3: 100% of transactions received via our digital experiences are either processed correctly or we have notified the user of an error.
Veteran Journey
Getting out: Engaging VA to access benefits and services Taking care of myself: Managing primary care and chronic health issues
Research Goals
1. Evaluate key usability improvements in the Apple Prototype
Determine if recent design and content changes have improved the overall user experience for Veterans
Assess scanability and comprehension of content changes 
Assess whether date field modifications have reduced friction points for Veterans
Reevaluate usefulness of laterality question 
2. Compare effectiveness of Mango vs. Kiwi design approaches
Determine which design approach better supports accurate claim type identification
Identify which design better aligns with Veterans' mental models and expectations
Assess which approach provides a more intuitive editing experience
3. Assess the holistic user journey through the conditions step
Understand if the end-to-end flow provides a cohesive and intuitive experience
Evaluate whether recent copy changes support Veterans' decision-making
Identify remaining pain points in the conditions step 

Outcome
This research will:
Inform the next iteration of the 526EZ form design across all tested components
Provide insights to reduce claim processing errors
Help streamline the Veterans' claims submission experience
Guide implementation decisions between Mango and Kiwi design approaches
Research questions
Goal 1: Apple Prototype Updates
How has improved scanability on key screens affected Veterans' ability to process information?
How do Veterans interact with formData examples on cause details screens, and do these examples improve comprehension?
What impact does adding specific terminology ("disease onset"/"event") have on Veterans' understanding of cause questions?
How does removing the "required" label from date fields while including the day component affect completion rates and accuracy?
How do Veterans experience the laterality question? Is it straightforward or cause confusion?
What impact does the multi-page approach have on perceived form length and complexity?
Measured by: Completion rates, observation notes, verbal comments during think-aloud protocol, post-task interview questions, error rates, accuracy of information provided
Goal 2: Mango vs. Kiwi Design Comparison
Which design do Veterans prefer and why? 
Which design aligns better to Veteran’s mental model on conditions/disabilities? 
Does one design better support accurate condition entry and classification? 
What challenges do Veterans encounter when going through each flow?
Does editing follow-up questions on a specific condition create confusion or clarity within the edit mode of the multiple-response pattern?
Measured by: Preference question, task completion rates, self-reported confidence, verbal feedback 
Goal 3: End-to-End Flow of the Conditions Step
What is the Veterans' experience navigating from the introduction page to the review page?
How effective are the copy changes in improving wayfinding throughout the conditions step, and does each page feel like a logical next step in the process?
What points in the flow cause confusion or hesitation for Veterans?
How confident do Veterans feel about their selections upon reaching the review page?
Do Veterans understand how to navigate back to modify previously entered information?
Measured by: Edit task completion rates, verbal expressions of certainity/doubt, verbal feedback 

How will we measure? 
Goal 1: Apple Prototype Changes
Task completion: Ability to successfully add multiple conditions, including date, laterality, and cause information
Scanability & instructions: Observation on whether Veterans read and understand instructions and page content
Context understanding: Observation and qualitative feedback on effectiveness of formData in helping Veterans identify which condition they're entering/editing
Comprehension: Ability to understand what each page is asking them to provide
Date component: Identification of friction points during date entry and success with approximate dates
Error handling: Ability to recognize and self-correct errors without external prompting
Goal 2: Mango vs. Kiwi Design Comparison:
Design preference: Direct comparison, preference strength ratings, and qualitative reasoning
Mental model:  Observations of how Veterans conceptualize and categorize their conditions
Goal 3: End-to-End Flow of the Conditions Step
Navigation experience: Identification of hesitation points and logical flow understanding
Edit experience: Success rates of edit attempts and qualitative feedback on the process
Confidence check: Self-reported certainty in selections upon reaching review page
Hypotheses
Apple Prototype Updates
We hypothesize that Veterans will find the updated Apple prototype easier to use compared to previous versions, specifically:
The addition of hint text in the date component will reduce confusion about what information is being requested, but the inclusion of a day field may create new confusion for Veterans who don't recall the specific day of an event or onset.
Including the condition name (via formData) will improve context and reduce errors when Veterans are completing cause details screens.
Reduced examples and refined language will improve scanability, leading Veterans to review all four cause question options before selecting
Mango vs. Kiwi Design
Mango design will be more intuitive because it:
ensures a review of existing disabilities, provides upfront visibility of rated disabilities, and reduces confusion around claim type terminology.
Is easier to edit due to its integrated approach to displaying all condition types together.
Methodology
Testing Structure
Each session will use a mixture of participants' real conditions and pre-defined mock scenarios
We will use an A/B testing approach to compare Mango and Kiwi designs with counterbalancing to control for order effects
Sessions will include time for task completion, feedback, and semi-structured interview questions
Participants will be tasked with completing Step 2 from the introduction page up to the review page, alternating between Mango and Kiwi designs. 
Data Collection
Record task completion rates and times
Capture qualitative feedback through think-aloud protocol
Use post-task satisfaction ratings
Collect preference data between competing designs
Accessibility Considerations
Due to the complexity of what we're testing in this round, we will not include Assistive Technology (AT) users in this phase. After selecting and refining either the Mango or Kiwi design, we will conduct a separate round of testing focused on accessibility with AT users to validate the changes made. 
Analysis Plan
Compare task completion and preference metrics between Mango and Kiwi designs
Identify common pain points across the end-to-end flow
Evaluate the effectiveness of specific updates to the Apple prototype
Generate actionable recommendations for the next design iteration
Location
Remote interviews on Zoom
Recruitment approach
We will recruit 12 Veterans for a minimum of 8 sessions
Primary Criteria
All participants are receiving disability compensation from the VA for at least two conditions 
At least 1 but no more than 2 Veterans who identify as having a cognitive disability.
At least 1 but no more than 3 Veterans over 55 years old 
Education
At least 1 but no more than 3 “no degree after high school” 
Secondary Criteria
We’d like a mix of genders, age, and race/ethnicity 
Gender
Attempt At least 2 but no more than 3 Veterans who identify as a gender other than male 
Age
Attempt at least 1 Veteran under 35 years old 
Race/ethnicity
Attempt at least 2 but no more than 4 Veterans who are not white/Caucasian
Screener questions
General: Disability compensation 
Have you ever filed a claim for disability compensation with the VA in the past?
Yes [Recruit for all] 
No [Remove] 
2. Are you currently receiving monthly disability compensation? 
Yes [Recruit for all]
No [Remove] 


Length of sessions
Session length: 75-90 minutes, schedule for 90 
Buffer time between sessions: 30 minutes 
Maximum Sessions per day: 2 sessions 



OUT OF SCOPE 


Availability
When would you like sessions scheduled? Please list exact dates and times in EASTERN Standard Time. Note: we recommend providing availability outside of work hours, as many Veterans are only available before and after working times, and live across the U.S.
Please request enough dates and at least double the amount of time slots for the number of requested participants. (e.g. Monday 9-1, 3-6; Tuesday 9-6, etc.; 12 time slots for 6 participants). This helps Perigean book participants when there are more time slots available, and when sessions need to be rescheduled or filled in with further recruitment.


Timeline
Please submit artifacts for Research Review 8-9 days prior to the first planned research day for remote studies so Perigean can begin recruiting one week prior. Perigean requires 2+ weeks for in-person.
TBD
Research materials
Note: your OCTO/VA lead must review and approve all research materials – including this plan – prior to submitting a recruitment request.
Provide a link to any materials you need to run your study, including any materials needed for set up and recruitment.
For moderated interviews:
[Link to conversation guide](url goes here)
For moderated usability tests:
[Link to conversation guide](url goes here)
[Link to prototype](url goes here)
For unmoderated testing:
[Link to email with instructions](url goes here)
[Link to prototype or OptimalSort session for group A](url goes here)
[Link to prototype or OptimalSort session for group B](url goes here)
If recruiting outside of Perigean’s participant database:
[Link to recruitment flyer](url goes here)
Prepare
When will the thing you are testing be finalized? Ideally it's ready a week before testing begins and has also been through a Midpoint review.
A pilot session is required. Please indicate the date and name of a mock participant for a pilot session.
Pilot participant email:
Date and time of pilot session:
Research sessions
Planned dates of research:


Team Roles
Please do not include email addresses in this section. We previously required email addresses. VA's GitHub policy (see announcement) has changed. VA.gov email addresses cannot be in public repositories.
Please list the names of people in each role. In the Slack study channel, send an email and primary phone number for the moderator. Send emails only for the notetaker, accessibility specialist, and observers. If you need Perigean to take notes for you, indicate that next to Notetaker.
Moderator:
Research guide writing and task development (usually but not always same as moderator):
Participant recruiting & screening:
Project point of contact:
Participant(s) for pilot test:
Accessibility specialist (for sessions where support for assistive technology may be needed):
Note-takers:
Observers: List the names of people observing the sessions. This includes VA stakeholders, engineering team members, design team members, and any other people who might find this research relevant to their work. Spread observers across sessions. There should be no more than 5 to 6 total attendees (moderator, notetaker(s), observer(s)) per session on the VA side.
