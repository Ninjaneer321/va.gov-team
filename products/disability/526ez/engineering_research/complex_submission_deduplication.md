# 'Complex' de-duping (or grouping by sameness when it's hard)

'De-duping' is the catch-all term we are using to describe our high level goal of [removing duplicate 526 submissions from consideration](https://github.com/department-of-veterans-affairs/va.gov-team/issues/80624) relative to our 'untouched submission audit'. Within this document, what is actually being discussed is a subset of that problem, wherein we want to group submissions, scoped to a single veteran, by 'sameness'. More specifically, grouping submissions by sameness in the event of multiple, inconsistant differences across those submissions, i.e. 'complex' differences. These duplicate sets (**dupe-sets** for short) will then be put through a rule engine to determine which require remediation and which can be ignored.

## Axioms
- GIVIN a set of submissions, scoped to a single user
- IF one submission is identical to another across all values it can be said to be a true duplicate of that submission
- WHEN muptiple submission are identical across *all* form values, they can be said to be a true duplicate set.
- A 'dupe set' can be as small as a single submission.
- 'dupe sets' can only ever get smaller when we introduce more differences. 

**In this document, when I talk about "sameness", this is what I'm talking about.**

## Out of scope
Samness-grouping (the slice of de-duping we are looking at), simple or comples, does not apply rules about which member of a dupe-set should be investigated and which can be ignored.  The results of 'complex' deduping will be passed to our `TimeAndStatus` sorter to make these decisions. [TODO - document this object]

## The Diff Report

**Diff Reports** are generated by the `SubmissionDifferenceReport` object, [documented here.](https://github.com/department-of-veterans-affairs/va.gov-team-sensitive/blob/master/teams/benefits/scripts/526/submission_difference_report.rb)

This is similar to a git diff, where we are identifying difference. The following logic assumes that given a set of submissions (scoped to a user) we have a diff report available.  The 'sameness grouping', simple and complex, analyzes this diff report and groups the affected submissions into dupe-sets.

### How it works
The Diff Report returns a hash of 'key chains' and their respective data.  A key chain is an array of hash keys that describes the location of a variation within a set of 526 submissions.  For example, when capturing the variants in the following form data:

```
submissionID 1
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant A>
      }
    }
  }
}

submissionID 2
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant>
  }
}

submissionID 3
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
}
```

The diff report would return the following keychain and data:

```ruby
{ 
  ['form526', 'form526', 'veteran', 'mailingAddress'] => {
    'variant A' => [<submission 1 id>],
    'variant B' => [<submission 2 id>, <submission 3 id>]
  }
}
```

The values are hashes where the variation (String) points to an array of submissions, from the relevant set, that contain that variant. The Diff generator finds these deeply nested variations by flattening the form json and simply comparing key chains and the strings they return.

## 'Simple' de-duping (sameness grouping)
Our 'simple sameness grouper' was a first pass at processing a duplicate report and grouping the scoped submissions into groups of sameness. [TODO - link this].  This works great if there is only one variation accross all submissions of the set. However, its common for 3 or more submissions to diverge on multiple values, and in unique ways from submission to submission. 

### 'Complex' de-duping (sameness grouping when the differences are more complex)

In the example below you can see that our previous grouping of `[<submission 2 id>, <submission 3 id>]` based on `variant B` is no longer accurate.  This duplicate set needs to be futher broken down based on the secondary variant.

```ruby
submissionID 1
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant A>
      }
    }
  }
}

submissionID 2
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant C>
  }
}

submissionID 3
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant D>
  }
}
```

Now our duplicate report will look like this:

```ruby
{ 
  ['form526', 'form526', 'veteran', 'mailingAddress'] => {
    'variant A' => [<submission 1 id>],
    'variant B' => [<submission 2 id>, <submission 3 id>]
  }
  ['form4142'] => {
    'variant C' => [<submission 2 id>],
    'variant D' => [<submission 3 id>]
  }
}
```

It's still pretty easy to see quickly that these three submissions must be each treated as single-member dupe-sets. However, when we are looking at 5, 10, even 20 submissions for a user, often many differences across many different overlapping subsets, grouping submissions by sameness quickly becomes complex. 

Something to keep in mind as we ratchet up the complexity is that dupe-sets only ever get smaller.  If create dupe sets based on a single difference, then those already differentiated submissions will never become *more* similar by introducing new differences. Using this Axiom we can begin to develop an algorythm that itterataivly breaks down dupe-sets into smaller dupe-sets.  

In the contrived example below, you can see how we will break down a multi-submission, multi-variation submission set into dupe sets.

```ruby
# given submissions [1,2,3,4,5,6]
{
  <user uuid> => {
    <key chain 1> => {
      <variant> => [1, 2, 3, 4]
      <variant> => [5, 6]
    },
    <key chain 2> => {
      <variant> => [2, 3, 5]
      <variant> => [1]
      <variant> => [4, 6]
    },
    <key chain 3> => {
      <variant> => [2, 3, 5]
      <variant> => [1]
      <variant> => [4, 6]
    }
  }
}
```

Note neither that the content of a given variant nor the keychain within which it was identified is imporant here. We can think of this data as nothing more than sets of dupe sets. Given this scope, we can simplify our data to a 3 level array.

#### If we cared about the meaning of variant / key chain (we don't)

|             | variant   | variant | variant | variant | variant | variant | variant | variant |
|-------------|-----------|---------|---------|---------|---------|---------|---------|---------|
| key chain 1 | [1,2,3,4] | [5,6]   |         |         |         |         |         |         |
| key chain 2 |           |         | [2,3,5] | [1]     | [4,6]   |         |         |         |
| key chain 3 |           |         |         |         |         | [1,4]   | [2,3]   | [5,6]   |


#### Simplified for our purpose

```
[
  [[1,2,3,4], [5,6]],
  [[2,3,5], [1], [4,6]],
  [1,4], [2,3], [5,6]],
]
```

Using this simplifed version, we can step through, itteratively comparing dupe sets, leaving only dupe sets present in each set of dupe sets. If we notate our bottom level arrays as such

```ruby
[
comparative set -->  [A:[1,2,3,4], B:[5,6]],
itterative foucs --> [C:[2,3,5], D:[1], E:[4,6]],
                     [F:[1,4], G:[2,3], H:[5,6]]
]
```

Then the first (deep) itteration of our loop would be something like this

1. our first dupe set-set is isolated as our 
2. Itteration begins on the remaining objects
  - Largest subset of C and A = [2,3]
  - Largest subset of C and B = [5]
  - Largest subset of D and A = [1]
  - Largest subset of D and B = []
  - Largest subset of E and A = [4]
  - Largest subset of E and B = [6]
3. The result of this itteration is our new 'most accurate' set of dupe sets: `[[2,3], [5], [1], [4], 6]]`. With only one itteration we've reduced the nubmer of dupe sets to 1.  For our next itteration, we will do the same thing, using this 'most accurate' set as our comparative set.

```ruby
[
comparative set -->  [A:[2,3], B:[5], C:[1], D:[4], E:[6]],
itterative focus --> [F:[1,4], G:[2,3], H:[5,6]]
]
```
Our second itteration proceeds thusly:

  - Largest subset of F and A = []
  - Largest subset of F and B = []
  - Largest subset of F and C = [1]
  - Largest subset of F and D = [4]
  - Largest subset of F and E = []
  - Largest subset of G and A = [2,3] 
  - Largest subset of G and B = [] 
  - Largest subset of G and C = []
  - Largest subset of G and D = []
  - Largest subset of G and E = []
  - Largest subset of H and A = []
  - Largest subset of H and B = [5]
  - Largest subset of H and C = []
  - Largest subset of H and D = []
  - Largest subset of H and E = [6]

Our result is `[[1], [2,3], [4], [5], [6]`.  Our dupe set of submission `[2,3]` survived, meaning it is a true duplicate. the other submissions will all be marked as 'invetigate' in our audit, since they are not true duplicates.


Note that if we cared to, we could optimize this to not do so many worthless itterations.  However, these Duplicate reports tend to only have a few key chains, and submission sets tend to top out in the teens. For this reason, we are sacrificing performance for time.  Also, if all goes well, this should only ever run a few times.

Note that the hard work is happening in application memory. small DB queries are fired, but nothing that risks locking our database, even if this script takes days to run.
